{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UserId': 'AROA5BZFIZOADUUEBUTW6:caleb.chan@nxp.com', 'Account': '897189464960', 'Arn': 'arn:aws:sts::897189464960:assumed-role/AWSReservedSSO_ccoe-powerusers_6dbe169efbadd99d/caleb.chan@nxp.com', 'ResponseMetadata': {'RequestId': '462ed97b-5931-4d24-8629-ac0d8c21ee2f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '462ed97b-5931-4d24-8629-ac0d8c21ee2f', 'x-amz-sts-extended-request-id': 'MTp1cy1lYXN0LTE6MTc1NDQ3MDgxNzYxNTpHOnd6bVNQbU5z', 'content-type': 'text/xml', 'content-length': '490', 'date': 'Wed, 06 Aug 2025 09:00:17 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "session = boto3.Session(profile_name=\"Caleb\")\n",
    "\n",
    "sts = session.client(\"sts\")\n",
    "identity = sts.get_caller_identity()\n",
    "\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current AWS Profile: Caleb\n",
      "AWS Identity: {'UserId': 'AROA5BZFIZOADUUEBUTW6:caleb.chan@nxp.com', 'Account': '897189464960', 'Arn': 'arn:aws:sts::897189464960:assumed-role/AWSReservedSSO_ccoe-powerusers_6dbe169efbadd99d/caleb.chan@nxp.com', 'ResponseMetadata': {'RequestId': 'b6199c1c-e6c0-4a58-9edb-b95bcdcf2b28', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b6199c1c-e6c0-4a58-9edb-b95bcdcf2b28', 'x-amz-sts-extended-request-id': 'MTp1cy1lYXN0LTE6MTc1NDQ3MDgyMjYwNTpHOkF1WldlSVB4', 'content-type': 'text/xml', 'content-length': '490', 'date': 'Wed, 06 Aug 2025 09:00:22 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# aws sso login --profile Caleb\n",
    "# aws sts get-caller-identity --profile Caleb\n",
    "\n",
    "# Get current profile from environment\n",
    "profile = os.environ.get(\"AWS_PROFILE\", \"Caleb\")\n",
    "print(\"Current AWS Profile:\", profile)\n",
    "\n",
    "session = boto3.Session(profile_name=profile)\n",
    "\n",
    "# Verify identity\n",
    "sts = session.client(\"sts\")\n",
    "identity = sts.get_caller_identity()\n",
    "print(\"AWS Identity:\", identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing SSO -> Role -> Bedrock Flow\n",
      "=============================================\n",
      "1. Testing SSO profile: Caleb\n",
      "   ‚úÖ SSO profile works!\n",
      "   Identity: arn:aws:sts::897189464960:assumed-role/AWSReservedSSO_ccoe-powerusers_6dbe169efbadd99d/caleb.chan@nxp.com\n",
      "   Account: 897189464960\n",
      "\n",
      "2. Assuming role: arn:aws:iam::897189464960:role/rag\n",
      "   ‚úÖ Role assumption successful!\n",
      "\n",
      "3. Testing Bedrock access...\n",
      "   ‚ùå Bedrock failed: An error occurred (AccessDeniedException) when calling the InvokeModel operation: You don't have access to the model with the specified model ID.\n",
      "   üí° Add Bedrock permissions to your role\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Your SSO profile name\n",
    "SSO_PROFILE = 'Caleb'\n",
    "ROLE_ARN = 'arn:aws:iam::897189464960:role/rag'\n",
    "\n",
    "def test_sso_bedrock():\n",
    "    \"\"\"Test the complete SSO -> Role -> Bedrock flow\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Testing SSO -> Role -> Bedrock Flow\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Test SSO profile\n",
    "        print(f\"1. Testing SSO profile: {SSO_PROFILE}\")\n",
    "        session = boto3.Session(profile_name=SSO_PROFILE)\n",
    "        sts = session.client('sts')\n",
    "        \n",
    "        identity = sts.get_caller_identity()\n",
    "        print(f\"   ‚úÖ SSO profile works!\")\n",
    "        print(f\"   Identity: {identity['Arn']}\")\n",
    "        print(f\"   Account: {identity['Account']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå SSO profile failed: {e}\")\n",
    "        print(f\"   üí° Try: aws sso login --profile {SSO_PROFILE}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Step 2: Assume the role\n",
    "        print(f\"\\n2. Assuming role: {ROLE_ARN}\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=ROLE_ARN,\n",
    "            RoleSessionName='bedrock-session',\n",
    "            DurationSeconds=3600\n",
    "        )\n",
    "        \n",
    "        credentials = response['Credentials']\n",
    "        print(\"   ‚úÖ Role assumption successful!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Role assumption failed: {e}\")\n",
    "        print(\"   üí° Check role trust policy - it needs to allow your SSO role\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Step 3: Test Bedrock\n",
    "        print(\"\\n3. Testing Bedrock access...\")\n",
    "        client = boto3.client(\n",
    "            'bedrock-runtime',\n",
    "            region_name='us-east-1',  # Try us-east-1 first\n",
    "            aws_access_key_id=credentials['AccessKeyId'],\n",
    "            aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "            aws_session_token=credentials['SessionToken']\n",
    "        )\n",
    "        \n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-25\",\n",
    "            \"max_tokens\": 100,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Hello! Please confirm you're working through AWS Bedrock.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response = client.invoke_model(\n",
    "            modelId='anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read().decode('utf-8'))\n",
    "        print(\"   ‚úÖ Bedrock works!\")\n",
    "        print(f\"   Claude says: {response_body['content'][0]['text']}\")\n",
    "        \n",
    "        print(\"\\nüéâ SUCCESS! Everything is working!\")\n",
    "        return client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Bedrock failed: {e}\")\n",
    "        \n",
    "        # Try to give specific guidance\n",
    "        if 'ValidationException' in str(e):\n",
    "            print(\"   üí° Request access to Claude models in Bedrock console\")\n",
    "        elif 'AccessDenied' in str(e):\n",
    "            print(\"   üí° Add Bedrock permissions to your role\")\n",
    "        else:\n",
    "            print(\"   üí° Check region availability (try us-east-1)\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "def create_working_client():\n",
    "    \"\"\"Create a working Bedrock client for future use\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*45)\n",
    "    print(\"üìã Creating reusable Bedrock client...\")\n",
    "    \n",
    "    try:\n",
    "        # Assume role\n",
    "        session = boto3.Session(profile_name=SSO_PROFILE)\n",
    "        sts = session.client('sts')\n",
    "        \n",
    "        response = sts.assume_role(\n",
    "            RoleArn=ROLE_ARN,\n",
    "            RoleSessionName='bedrock-work-session',\n",
    "            DurationSeconds=3600\n",
    "        )\n",
    "        \n",
    "        credentials = response['Credentials']\n",
    "        \n",
    "        # Create client\n",
    "        client = boto3.client(\n",
    "            'bedrock-runtime',\n",
    "            region_name='us-east-1',\n",
    "            aws_access_key_id=credentials['AccessKeyId'],\n",
    "            aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "            aws_session_token=credentials['SessionToken']\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Client created! Use this for your Bedrock calls:\")\n",
    "        print(\"\"\"\n",
    "# Your working client code:\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def get_bedrock_client():\n",
    "    session = boto3.Session(profile_name='ccoe-powerusers-382209588990')\n",
    "    sts = session.client('sts')\n",
    "    \n",
    "    response = sts.assume_role(\n",
    "        RoleArn='arn:aws:iam::897189464960:role/rag',\n",
    "        RoleSessionName='bedrock-session',\n",
    "        DurationSeconds=3600\n",
    "    )\n",
    "    \n",
    "    credentials = response['Credentials']\n",
    "    \n",
    "    return boto3.client(\n",
    "        'bedrock-runtime',\n",
    "        region_name='us-east-1',\n",
    "        aws_access_key_id=credentials['AccessKeyId'],\n",
    "        aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "        aws_session_token=credentials['SessionToken']\n",
    "    )\n",
    "\n",
    "# Usage:\n",
    "client = get_bedrock_client()\n",
    "# ... make your Bedrock calls with client\n",
    "        \"\"\")\n",
    "        \n",
    "        return client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create client: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the flow\n",
    "    client = test_sso_bedrock()\n",
    "    \n",
    "    if client:\n",
    "        # Provide reusable code\n",
    "        create_working_client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Invocation (GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-f6ac42e5', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '<reasoning>We need to respond as ChatGPT with a short self-introduction, following the style guidelines: short, concise, friendly, no emojis, no excessive detail. Should be about 2-3 sentences.</reasoning>I‚Äôm ChatGPT, an AI language model created by OpenAI. I can help answer questions, brainstorm ideas, and assist with a wide range of topics‚Äîjust let me know what you need!', 'role': 'assistant'}}], 'created': 1754471209, 'model': 'openai.gpt-oss-120b-1:0', 'service_tier': 'standard', 'system_fingerprint': 'fp_bc33001493', 'object': 'chat.completion', 'usage': {'completion_tokens': 96, 'prompt_tokens': 18, 'total_tokens': 114}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "session = boto3.Session(profile_name=\"Caleb\")\n",
    "bedrock = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "response = bedrock.invoke_model(\n",
    "    modelId=\"openai.gpt-oss-120b-1:0\",\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\",\n",
    "    body=json.dumps({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello, tell me about youself!\"\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_completion_tokens\": 1024\n",
    "    })\n",
    ")\n",
    "\n",
    "response_body = json.loads(response['body'].read())\n",
    "print(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant's Response:\n",
      "<reasoning>The user says: \"Hello, tell me about yourself!\" They want a self-introduction. According to policy, we can comply. It's a simple request. So we can give a brief self-introduction. Should be short. The user wants a short self-introduction. So we respond with a short self-introduction.</reasoning>Sure! I‚Äôm ChatGPT, an AI language model created by OpenAI. I‚Äôm designed to understand and generate human‚Äëlike text, answer questions, help with writing, brainstorm ideas, and chat about a wide range of topics. I don‚Äôt have personal experiences or feelings, but I‚Äôm here to assist you with information, creativity, and problem‚Äësolving whenever you need it.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model used: openai.gpt-oss-120b-1:0\n",
      "Finish reason: stop\n",
      "Total tokens used: 187\n",
      "Input tokens: 18\n",
      "Output tokens: 169\n"
     ]
    }
   ],
   "source": [
    "assistant_message = response_body['choices'][0]['message']['content']\n",
    "print(\"Assistant's Response:\")\n",
    "print(assistant_message)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(f\"Model used: {response_body['model']}\")\n",
    "print(f\"Finish reason: {response_body['choices'][0]['finish_reason']}\")\n",
    "print(f\"Total tokens used: {response_body['usage']['total_tokens']}\")\n",
    "print(f\"Input tokens: {response_body['usage']['prompt_tokens']}\")\n",
    "print(f\"Output tokens: {response_body['usage']['completion_tokens']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Invocation (Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=\"Caleb\") \n",
    "client = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "response = client.invoke_model_with_response_stream(\n",
    "    #modelId='arn:aws:bedrock:eu-west-1:897189464960:inference-profile/eu.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "    modelId='arn:aws:bedrock:us-west-2:897189464960:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
    "    contentType='application/json',\n",
    "    accept='application/json',\n",
    "    body=json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"\"\"\"\n",
    "                    Hello\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    ")\n",
    "\n",
    "for event in response['body']:\n",
    "    try:\n",
    "        chunk = json.loads(event['chunk']['bytes'].decode())\n",
    "        if 'delta' in chunk:\n",
    "            try:\n",
    "                print(chunk['delta']['text'], end='', flush=True)\n",
    "            except KeyError:\n",
    "                pass  # skip chunks without 'text'\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing chunk: {e}\")\n",
    "\n",
    "    \n",
    "#print(response['body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence transformer embeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "texts = ['hello','i am snowy']\n",
    "embeddings = similarity_model.encode(texts, convert_to_tensor=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "session = boto3.Session(profile_name=\"Caleb\")\n",
    "bedrock = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\",client=bedrock)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Chunking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf; by character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion():\n",
    "    loader=PyPDFDirectoryLoader(\"data\")\n",
    "    documents=loader.load()\n",
    "\n",
    "    # - in our testing Character split works better with this PDF data set\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=10000,\n",
    "                                                 chunk_overlap=1000)\n",
    "    \n",
    "    docs=text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 4 document chunks\n",
      "\n",
      "First chunk preview:\n",
      "#include \"Testplan.h\"\n",
      "//-----------------------------------------------------------------------------\n",
      "//\n",
      "// Task: Func\n",
      "//\n",
      "// Description:\n",
      "// \n",
      "// Revision history:\n",
      "//\n",
      "//-----------------------------------------------------------------------------\n",
      "TASK_FUNCTION Func ()\n",
      "{\n",
      "  DWORD TestNumber;\n",
      "   double VDD;\n",
      "  \n",
      "  BEGIN_TASK (FUNC_ID)\n",
      "    // Update the die list\n",
      "    GetDieList (gDieList);\n",
      "    //Connect grounds to Abus ground for all tests\n",
      "      GndConnectAbus    (ROW4);\n",
      "      PinConnectAbus    (pinVSS,...\n",
      "Metadata: {'producer': '', 'creator': '', 'creationdate': '2025-08-06T03:03:30-07:00', 'title': '', 'author': 'Perapach Nimkuntod', 'subject': '', 'keywords': '', 'moddate': '2025-08-06T03:03:30-07:00', 'source': 'data/PCA9546ABS_TP Comparison.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure you have pandas installed: pip install pandas openpyxl\n",
    "    docs = data_ingestion()\n",
    "    print(f\"Loaded and split {len(docs)} document chunks\")\n",
    "    \n",
    "    # Print first chunk as example\n",
    "    if docs:\n",
    "        print(\"\\nFirst chunk preview:\")\n",
    "        print(docs[0].page_content[:500] + \"...\")\n",
    "        print(f\"Metadata: {docs[0].metadata}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv; by character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 4 document chunks\n",
      "\n",
      "First chunk preview:\n",
      "File: PCA9546ABS_TP Comparison(FUNC).csv\n",
      "Sheet: Sheet1\n",
      "Rows: 282, Columns: 13\n",
      "--------------------------------------------------\n",
      "Columns: #include \"Testplan.h\", Unnamed: 1, Unnamed: 2, Unnamed: 3, Unnamed: 4, Unnamed: 5, Unnamed: 6, Unnamed: 7, Unnamed: 8, Unnamed: 9, Unnamed: 10, Unnamed: 11, Unnamed: 12\n",
      "--------------------------------------------------\n",
      "Row 2: #include \"Testplan.h\": //-----------------------------------------------------------------------------\n",
      "Row 3: #include \"Testplan.h\": //...\n",
      "Metadata: {'source': 'PCA9546ABS_TP Comparison(FUNC).csv', 'sheet': 'CSV', 'type': 'csv'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def data_ingestion():\n",
    "    \"\"\"Load and split Excel files from the data directory\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Get all Excel files from the data directory\n",
    "    data_dir = \"data\"\n",
    "    excel_extensions = ['.xlsx', '.xls', '.csv']\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if any(filename.lower().endswith(ext) for ext in excel_extensions):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            \n",
    "            try:\n",
    "                # Read Excel file (handles both .xlsx and .xls)\n",
    "                if filename.lower().endswith('.csv'):\n",
    "                    df = pd.read_csv(file_path)\n",
    "                else:\n",
    "                    df = pd.read_excel(file_path, sheet_name=None)  # Read all sheets\n",
    "                \n",
    "                # Process multiple sheets if it's an Excel file\n",
    "                if isinstance(df, dict):  # Multiple sheets\n",
    "                    for sheet_name, sheet_df in df.items():\n",
    "                        content = convert_df_to_text(sheet_df, sheet_name, filename)\n",
    "                        if content.strip():  # Only add if content is not empty\n",
    "                            doc = Document(\n",
    "                                page_content=content,\n",
    "                                metadata={\n",
    "                                    \"source\": filename,\n",
    "                                    \"sheet\": sheet_name,\n",
    "                                    \"type\": \"excel\"\n",
    "                                }\n",
    "                            )\n",
    "                            documents.append(doc)\n",
    "                else:  # Single sheet or CSV\n",
    "                    content = convert_df_to_text(df, \"Sheet1\", filename)\n",
    "                    if content.strip():\n",
    "                        doc = Document(\n",
    "                            page_content=content,\n",
    "                            metadata={\n",
    "                                \"source\": filename,\n",
    "                                \"sheet\": \"Sheet1\" if not filename.endswith('.csv') else \"CSV\",\n",
    "                                \"type\": \"excel\" if not filename.endswith('.csv') else \"csv\"\n",
    "                            }\n",
    "                        )\n",
    "                        documents.append(doc)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Split documents using RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=10000,\n",
    "        chunk_overlap=1000\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def convert_df_to_text(df, sheet_name, filename):\n",
    "    \"\"\"Convert DataFrame to text format suitable for processing\"\"\"\n",
    "    if df.empty:\n",
    "        return \"\"\n",
    "    \n",
    "    # Create a structured text representation\n",
    "    text_parts = [\n",
    "        f\"File: {filename}\",\n",
    "        f\"Sheet: {sheet_name}\",\n",
    "        f\"Rows: {len(df)}, Columns: {len(df.columns)}\",\n",
    "        \"-\" * 50\n",
    "    ]\n",
    "    \n",
    "    # Add column headers\n",
    "    text_parts.append(\"Columns: \" + \", \".join(df.columns.astype(str)))\n",
    "    text_parts.append(\"-\" * 50)\n",
    "    \n",
    "    # Convert each row to text\n",
    "    for idx, row in df.iterrows():\n",
    "        row_text = []\n",
    "        for col, value in row.items():\n",
    "            if pd.notna(value):  # Only include non-null values\n",
    "                row_text.append(f\"{col}: {value}\")\n",
    "        \n",
    "        if row_text:  # Only add row if it has content\n",
    "            text_parts.append(f\"Row {idx + 1}: {' | '.join(row_text)}\")\n",
    "    \n",
    "    return \"\\n\".join(text_parts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv; by delimiter string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def data_ingestion():\n",
    "    \"\"\"Split Excel data by 'Test Number' delimiter\"\"\"\n",
    "    documents = []\n",
    "    data_dir = \"data\"\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.lower().endswith(('.xlsx', '.xls', '.csv')):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            \n",
    "            try:\n",
    "                # Read Excel/CSV file\n",
    "                if filename.lower().endswith('.csv'):\n",
    "                    sheets = {\"CSV\": pd.read_csv(file_path)}\n",
    "                else:\n",
    "                    sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "                \n",
    "                for sheet_name, df in sheets.items():\n",
    "                    if df.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Split by 'Test Number' delimiter\n",
    "                    test_chunks = split_by_test_number(df, sheet_name, filename)\n",
    "                    documents.extend(test_chunks)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def split_by_test_number(df, sheet_name, filename):\n",
    "    \"\"\"Split DataFrame by rows containing 'Test Number', including initial content\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Find all rows that contain 'Test Number' in any column\n",
    "    test_number_rows = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Check if any cell in the row contains 'Test Number'\n",
    "        row_contains_test = any(\n",
    "            str(cell).strip().startswith('TestNumber') \n",
    "            for cell in row.values \n",
    "            if pd.notna(cell)\n",
    "        )\n",
    "        \n",
    "        if row_contains_test:\n",
    "            test_number_rows.append(idx)\n",
    "    \n",
    "    # If no 'Test Number' found, treat entire sheet as one chunk\n",
    "    if not test_number_rows:\n",
    "        content = convert_entire_sheet_to_text(df, sheet_name, filename)\n",
    "        chunk = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": filename,\n",
    "                \"sheet\": sheet_name,\n",
    "                \"test_number\": \"No Test Number Found\",\n",
    "                \"chunk_type\": \"full_sheet\",\n",
    "                \"start_row\": 1,\n",
    "                \"end_row\": len(df)\n",
    "            }\n",
    "        )\n",
    "        return [chunk]\n",
    "    \n",
    "    # Handle content BEFORE first Test Number (if any)\n",
    "    if test_number_rows[0] > 0:\n",
    "        # There's content before the first Test Number\n",
    "        initial_chunk_df = df.iloc[0:test_number_rows[0]]\n",
    "        \n",
    "        content = convert_test_chunk_to_text(\n",
    "            initial_chunk_df, sheet_name, filename, \n",
    "            \"Header/Preamble\", 1, test_number_rows[0]\n",
    "        )\n",
    "        \n",
    "        chunk = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": filename,\n",
    "                \"sheet\": sheet_name,\n",
    "                \"test_number\": \"Header/Preamble\",\n",
    "                \"chunk_type\": \"initial_content\",\n",
    "                \"start_row\": 1,\n",
    "                \"end_row\": test_number_rows[0],\n",
    "                \"total_rows\": len(initial_chunk_df)\n",
    "            }\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Create chunks between Test Number markers\n",
    "    for i, start_row in enumerate(test_number_rows):\n",
    "        # Determine end row (next Test Number or end of sheet)\n",
    "        if i < len(test_number_rows) - 1:\n",
    "            end_row = test_number_rows[i + 1]\n",
    "        else:\n",
    "            end_row = len(df)\n",
    "        \n",
    "        # Extract chunk data\n",
    "        chunk_df = df.iloc[start_row:end_row]\n",
    "        \n",
    "        # Get test number from the first row\n",
    "        test_number = extract_test_number(chunk_df.iloc[0])\n",
    "        \n",
    "        # Convert chunk to text\n",
    "        content = convert_test_chunk_to_text(chunk_df, sheet_name, filename, test_number, start_row + 1, end_row)\n",
    "        \n",
    "        chunk = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": filename,\n",
    "                \"sheet\": sheet_name,\n",
    "                \"test_number\": test_number,\n",
    "                \"chunk_type\": \"test_based\",\n",
    "                \"start_row\": start_row + 1,\n",
    "                \"end_row\": end_row,\n",
    "                \"total_rows\": len(chunk_df)\n",
    "            }\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_test_number(first_row):\n",
    "    \"\"\"Extract the test number from the first row\"\"\"\n",
    "    for cell in first_row.values:\n",
    "        if pd.notna(cell) and 'Test Number' in str(cell):\n",
    "            # Try to extract the number part\n",
    "            cell_str = str(cell).strip()\n",
    "            # Examples: \"Test Number 1\", \"Test Number: 2\", \"Test Number 3.1\"\n",
    "            import re\n",
    "            match = re.search(r'Test Number[:\\s]*([0-9.]+)', cell_str, re.IGNORECASE)\n",
    "            if match:\n",
    "                return f\"Test Number {match.group(1)}\"\n",
    "            else:\n",
    "                return cell_str\n",
    "    return \"Unknown Test\"\n",
    "\n",
    "def convert_test_chunk_to_text(chunk_df, sheet_name, filename, test_number, start_row, end_row):\n",
    "    \"\"\"Convert a test chunk to structured text\"\"\"\n",
    "    content_parts = [\n",
    "        f\"File: {filename}\",\n",
    "        f\"Sheet: {sheet_name}\",\n",
    "        f\"Test: {test_number}\",\n",
    "        f\"Rows: {start_row}-{end_row}\",\n",
    "        f\"Columns: {', '.join(chunk_df.columns)}\",\n",
    "        \"=\" * 60\n",
    "    ]\n",
    "    \n",
    "    for idx, (original_idx, row) in enumerate(chunk_df.iterrows(), start=start_row):\n",
    "        row_parts = []\n",
    "        for col, value in row.items():\n",
    "            if pd.notna(value) and str(value).strip():\n",
    "                row_parts.append(f\"{col}: {value}\")\n",
    "        \n",
    "        if row_parts:\n",
    "            content_parts.append(f\"Row {original_idx + 1}: {' | '.join(row_parts)}\")\n",
    "    \n",
    "    return \"\\n\".join(content_parts)\n",
    "\n",
    "def convert_entire_sheet_to_text(df, sheet_name, filename):\n",
    "    \"\"\"Convert entire sheet when no Test Number found\"\"\"\n",
    "    content_parts = [\n",
    "        f\"File: {filename}\",\n",
    "        f\"Sheet: {sheet_name}\",\n",
    "        f\"Total Rows: {len(df)}\",\n",
    "        f\"Columns: {', '.join(df.columns)}\",\n",
    "        \"=\" * 60\n",
    "    ]\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        row_parts = []\n",
    "        for col, value in row.items():\n",
    "            if pd.notna(value) and str(value).strip():\n",
    "                row_parts.append(f\"{col}: {value}\")\n",
    "        \n",
    "        if row_parts:\n",
    "            content_parts.append(f\"Row {idx + 1}: {' | '.join(row_parts)}\")\n",
    "    \n",
    "    return \"\\n\".join(content_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 10 document chunks\n",
      "\n",
      "First chunk preview:\n",
      "File: PCA9546ABS_TP Comparison(FUNC).csv\n",
      "Sheet: CSV\n",
      "Test: Header/Preamble\n",
      "Rows: 1-30\n",
      "Columns: #include \"Testplan.h\", Unnamed: 1, Unnamed: 2, Unnamed: 3, Unnamed: 4, Unnamed: 5, Unnamed: 6, Unnamed: 7, Unnamed: 8, Unnamed: 9, Unnamed: 10, Unnamed: 11, Unnamed: 12\n",
      "============================================================\n",
      "Row 2: #include \"Testplan.h\": //-----------------------------------------------------------------------------\n",
      "Row 3: #include \"Testplan.h\": //\n",
      "Row 4: #include \"Testplan.h\": // ...\n",
      "Metadata: {'source': 'PCA9546ABS_TP Comparison(FUNC).csv', 'sheet': 'CSV', 'test_number': 'Unknown Test', 'chunk_type': 'test_based', 'start_row': 62, 'end_row': 88, 'total_rows': 27}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    docs = data_ingestion()\n",
    "    print(f\"Loaded and split {len(docs)} document chunks\")\n",
    "    \n",
    "    if docs:\n",
    "        print(\"\\nFirst chunk preview:\")\n",
    "        print(docs[0].page_content[:500] + \"...\")\n",
    "        print(f\"Metadata: {docs[2].metadata}\") # example chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'PCA9546ABS_TP Comparison(FUNC).csv', 'sheet': 'CSV', 'test_number': 'Unknown Test', 'chunk_type': 'test_based', 'start_row': 89, 'end_row': 115, 'total_rows': 27}, page_content='File: PCA9546ABS_TP Comparison(FUNC).csv\\nSheet: CSV\\nTest: Unknown Test\\nRows: 89-115\\nColumns: #include \"Testplan.h\", Unnamed: 1, Unnamed: 2, Unnamed: 3, Unnamed: 4, Unnamed: 5, Unnamed: 6, Unnamed: 7, Unnamed: 8, Unnamed: 9, Unnamed: 10, Unnamed: 11, Unnamed: 12\\n============================================================\\nRow 89: #include \"Testplan.h\":  TestNumber = 10003; | Unnamed: 12: TEST / SETUP #  :  1204         VCC    :  5.500  V     THRESHOLD  :  5.500  V\\nRow 90: #include \"Testplan.h\":     BEGIN_TEST (TestNumber) | Unnamed: 12: MEASURE         :  FUNCTION     CLAMP  :  NOT-USED     TDA VALUE  :  0.000  V\\nRow 91: Unnamed: 12: PARAMETER NAME  :  HI FCN       DELAY  :  10.00 MS     TDB VALUE  :  0.000  V\\nRow 92: #include \"Testplan.h\":       // Update the die list | Unnamed: 12: FUNC LINE ADDER :      0        OFFSET :  0.000  V     TTB - LOW  :  0.000  V\\nRow 93: #include \"Testplan.h\":       GetDieList (gDieList); | Unnamed: 12: FIRST FUNC LINE :   1593 PG 1   VILA   :  0.000  V     TTA - HIGH :  0.000  V\\nRow 94: Unnamed: 12: LAST  FUNC LINE :   1642 PG 1   VIHA   :  5.500  V     T0 CYCLE   :  10.00 US\\nRow 95: #include \"Testplan.h\": //Digital pattern test | Unnamed: 12: LOW  REJECT BIN :      2        VILB   :  1.650  V     T0 STOP    :  9.900 US\\nRow 96: Unnamed: 2: DigDriverSet  ( pinINPIN2, VDD, 0, RANGE_7V, gDieList ); | Unnamed: 12: HIGH REJECT BIN :      2        VIHB   :  3.850  V     TG START   :  1.000 US\\nRow 97: Unnamed: 2: DigDriverSet  ( pinINPIN3, 3.85, 1.65, RANGE_7V, gDieList ); | Unnamed: 12: F/M-VCC  OFFSET :  0.000  V     VOLA   :  1.400  V     TG STOP    :  8.000 US\\nRow 98: Unnamed: 12: FORCE     VALUE :  NOT-USED     VOHA   :  1.400  V     STROBE     :  9.900 US\\nRow 99: Unnamed: 2: DigSensorSet(pinINPIN1, 1.4, 1.4, RANGE_7V, gDieList ); | Unnamed: 12: LOW       LIMIT :  NOT-USED     VOLB   :  1.800  V     IOL VALUE  :  500.0 UA\\nRow 100: Unnamed: 2: DigSensorSet(pinSDA, 3.5, 1.8, RANGE_7V, gDieList ); | Unnamed: 12: HIGH      LIMIT :  NOT-USED     VOHB   :  3.500  V     IOH VALUE  : -500.0 UA\\nRow 101: Unnamed: 12: =======[ FILE NAME PCA9546A.FRB  GROUP #  53 ]=======  UNDER TEST :      1\\nRow 102: #include \"Testplan.h\":                 DigLoadSet( pinSDA, 0.5e-3, 0.5e-3, R40mA,VDD, gDieList ) ; | Unnamed: 12: F/M   : MACRO   = DISCONNECTED\\nRow 103: Unnamed: 12: FCN   : ALLPINS = 14 15  1  2 13  3  4  5  6  7  9 10 11 12 \\nRow 104: #include \"Testplan.h\":                 PinConnectDig ( pinAllpins, ALL, gDieList ); | Unnamed: 12: VINA  : INPIN2  = 14 15 \\nRow 105: Unnamed: 1:         PinConnectDigLoad ( pinSDA, gDieList ); | Unnamed: 12: VINB  : INPIN3  =  1  2  3 13 \\nRow 106: Unnamed: 12: VOUTA : INPIN1  =  1  3  2  4  6  9  5  7 10 12 11 14 15 13 \\nRow 107: #include \"Testplan.h\":         RunD (\"VIH ADR/RST\", RNORM, &PattErr, NC_STEP, NC_STEP, TM_LOAD ); | Unnamed: 12: VOUTB : OUTPIN1 = 15 \\nRow 108: #include \"Testplan.h\":         StoreDigResult( TestNumber, &PattErr, gDieList ); | Unnamed: 12: LOAD  : OUTPIN1 = 15 \\nRow 109: Unnamed: 12: EXT   : MACRO   = DISCONNECTED\\nRow 110: #include \"Testplan.h\":                 PinDisconnectDig ( pinAllpins, ALL, gDieList ); | Unnamed: 12: NOTE  : CHK VIH ADR/RESET\\nRow 111: #include \"Testplan.h\":                 PinDisconnectDigLoad ( pinSDA, gDieList );\\nRow 112: #include \"Testplan.h\":     END_TEST')"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(docs):\n",
    "    # convert to embeddings\n",
    "    vectorstore_faiss=FAISS.from_documents(\n",
    "        docs,\n",
    "        bedrock_embeddings # titan model\n",
    "    )\n",
    "    vectorstore_faiss.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data_ingestion()\n",
    "get_vector_store(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading vdb from local directory\n",
    "faiss_index = FAISS.load_local(\"faiss_index\", bedrock_embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_llm():\n",
    "    llm = Bedrock(model_id=\"openai.gpt-oss-120b-1:0\", client=bedrock)\n",
    "    return llm\n",
    "\n",
    "def get_anthropic_llm():\n",
    "    llm=Bedrock(model_id=\"anthropic.claude-sonnet-4-20250514-v1:0\", client=bedrock)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_oss = get_openai_llm()\n",
    "claude_sonnet = get_anthropic_llm()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: You are an expert in semiconductor test automation and script translation. Your task is to convert legacy MCT 2000 test scripts into the modern SPEAL format. You must maintain the logic, structure, and intent of the original script while rewriting it using the SPEAL syntax and conventions.\n",
    "You will receive an MCT 2000 code snippet as input, possibly accompanied by retrieved examples showing similar MCT-to-SPEAL conversions. Use these examples only as reference ‚Äî do not copy them directly unless clearly appropriate.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_template = (\n",
    "    \"You are an expert in semiconductor test automation and script translation. \"\n",
    "    \"Your task is to convert legacy MCT 2000 test scripts into the modern SPEAL format. \"\n",
    "    \"You must maintain the logic, structure, and intent of the original script while rewriting it using SPEAL syntax.\"\n",
    "    \"\\nUse the following context as reference if helpful. Code on the LEFT represents SPEAL formatting, while code on the RIGHT represents MCT 2000 formatting.\\n\"\n",
    ")\n",
    "\n",
    "human_template = \"\"\"\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an expert in semiconductor test automation and script translation. Your task is to convert legacy MCT 2000 test scripts into the modern SPEAL format. You must maintain the logic, structure, and intent of the original script while rewriting it using SPEAL syntax.\n",
      "Use the following context as reference if helpful. Code on the LEFT represents SPEAL formatting, while code on the RIGHT represents MCT 2000 formatting.\n",
      "\n",
      "Human: \n",
      "<context>\n",
      "some context\n",
      "</context>\n",
      "\n",
      "Question: Translate this code...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing prompt\n",
    "print(PROMPT.format(context=\"some context\", question=\"Translate this code...\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG (retrieval and LLM input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_llm(llm, vectorstore_faiss,query):\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "    answer=qa({\"query\":query})\n",
    "    return answer['result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_response_llm() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mWhat is this document about?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m get_response_llm(gpt_oss, faiss_index, user_input)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_response_llm() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "user_input = 'What is this document about?'\n",
    "get_response_llm(gpt_oss, faiss_index, user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: Failed to deserialize the JSON body into the target type: inputText: unknown field `inputText`, expected one of `messages`, `model`, `store`, `reasoning_effort`, `metadata`, `frequency_penalty`, `logit_bias`, `logprobs`, `top_logprobs`, `max_tokens`, `max_completion_tokens`, `n`, `modalities`, `prediction`, `audio`, `presence_penalty`, `response_format`, `seed`, `service_tier`, `stop`, `stream`, `stream_options`, `temperature`, `top_p`, `tools`, `tool_choice`, `parallel_tool_calls`, `user`, `function_call`, `functions` at line 1 column 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/langchain_community/llms/bedrock.py:546\u001b[0m, in \u001b[0;36mBedrockBase._prepare_input_and_invoke\u001b[0;34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49minvoke_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest_options)\n\u001b[1;32m    548\u001b[0m     text, body, usage_info \u001b[39m=\u001b[39m LLMInputOutputAdapter\u001b[39m.\u001b[39mprepare_output(\n\u001b[1;32m    549\u001b[0m         provider, response\n\u001b[1;32m    550\u001b[0m     )\u001b[39m.\u001b[39mvalues()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the InvokeModel operation: Failed to deserialize the JSON body into the target type: inputText: unknown field `inputText`, expected one of `messages`, `model`, `store`, `reasoning_effort`, `metadata`, `frequency_penalty`, `logit_bias`, `logprobs`, `top_logprobs`, `max_tokens`, `max_completion_tokens`, `n`, `modalities`, `prediction`, `audio`, `presence_penalty`, `response_format`, `seed`, `service_tier`, `stop`, `stream`, `stream_options`, `temperature`, `top_p`, `tools`, `tool_choice`, `parallel_tool_calls`, `user`, `function_call`, `functions` at line 1 column 12",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[264], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m user_prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mExplain what a qubit is.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m llm \u001b[39m=\u001b[39m get_openai_llm()\n\u001b[0;32m----> 8\u001b[0m llm\u001b[39m.\u001b[39;49minvoke([\n\u001b[1;32m      9\u001b[0m     SystemMessage(content\u001b[39m=\u001b[39;49msystem_prompt),\n\u001b[1;32m     10\u001b[0m     HumanMessage(content\u001b[39m=\u001b[39;49muser_prompt)\n\u001b[1;32m     11\u001b[0m ])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/langchain_core/language_models/llms.py:387\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[1;32m    379\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39minvoke\u001b[39m(\n\u001b[1;32m    380\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    386\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    389\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    390\u001b[0m             [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_input(\u001b[39minput\u001b[39m)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[39m.\u001b[39mtext\n\u001b[1;32m    401\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/langchain_core/language_models/llms.py:764\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[1;32m    756\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    757\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    762\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    763\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 764\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/langchain_core/language_models/llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m get_llm_cache() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    958\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    959\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m         )\n\u001b[1;32m    970\u001b[0m     ]\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    972\u001b[0m         prompts,\n\u001b[1;32m    973\u001b[0m         stop,\n\u001b[1;32m    974\u001b[0m         run_managers,\n\u001b[1;32m    975\u001b[0m         new_arg_supported\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(new_arg_supported),\n\u001b[1;32m    976\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    979\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    980\u001b[0m         callback_managers[idx]\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    981\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m missing_prompt_idxs\n\u001b[1;32m    989\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/langchain_core/language_models/llms.py:790\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    780\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    781\u001b[0m     prompts: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    787\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    788\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 790\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    791\u001b[0m                 prompts,\n\u001b[1;32m    792\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    793\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    794\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    795\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    796\u001b[0m             )\n\u001b[1;32m    797\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    798\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    799\u001b[0m         )\n\u001b[1;32m    800\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    801\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/langchain_core/language_models/llms.py:1544\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1542\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m   1543\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1544\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1545\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1546\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1547\u001b[0m     )\n\u001b[1;32m   1548\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m   1549\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/langchain_community/llms/bedrock.py:843\u001b[0m, in \u001b[0;36mBedrock._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m         completion \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m chunk\u001b[39m.\u001b[39mtext\n\u001b[1;32m    841\u001b[0m     \u001b[39mreturn\u001b[39;00m completion\n\u001b[0;32m--> 843\u001b[0m text, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_input_and_invoke(\n\u001b[1;32m    844\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/langchain_community/llms/bedrock.py:553\u001b[0m, in \u001b[0;36mBedrockBase._prepare_input_and_invoke\u001b[0;34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     text, body, usage_info \u001b[39m=\u001b[39m LLMInputOutputAdapter\u001b[39m.\u001b[39mprepare_output(\n\u001b[1;32m    549\u001b[0m         provider, response\n\u001b[1;32m    550\u001b[0m     )\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    552\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 553\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError raised by bedrock service: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m stop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     text \u001b[39m=\u001b[39m enforce_stop_tokens(text, stop)\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: Failed to deserialize the JSON body into the target type: inputText: unknown field `inputText`, expected one of `messages`, `model`, `store`, `reasoning_effort`, `metadata`, `frequency_penalty`, `logit_bias`, `logprobs`, `top_logprobs`, `max_tokens`, `max_completion_tokens`, `n`, `modalities`, `prediction`, `audio`, `presence_penalty`, `response_format`, `seed`, `service_tier`, `stop`, `stream`, `stream_options`, `temperature`, `top_p`, `tools`, `tool_choice`, `parallel_tool_calls`, `user`, `function_call`, `functions` at line 1 column 12"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_prompt = \"You are a helpful assistant that explains quantum concepts to 12-year-olds.\"\n",
    "user_prompt = \"Explain what a qubit is.\"\n",
    "\n",
    "llm = get_openai_llm()\n",
    "\n",
    "llm.invoke([\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt)\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import LLMResult, Generation\n",
    "from typing import Any, List, Optional, Dict\n",
    "import boto3\n",
    "import json\n",
    "from pydantic import Field\n",
    "\n",
    "# custom wrapper\n",
    "class GPTOSSLLM(BaseLLM):\n",
    "    bedrock: Any = Field(exclude=True) \n",
    "    model_id: str = \"openai.gpt-oss-120b-1:0\"\n",
    "    #model_id: str = \"anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "    #model_id: str = \"anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    \n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            try:\n",
    "                response = self.bedrock.invoke_model(\n",
    "                    modelId=self.model_id,\n",
    "                    contentType=\"application/json\",\n",
    "                    accept=\"application/json\",\n",
    "                    body=json.dumps({\n",
    "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        \"temperature\": kwargs.get(\"temperature\", 0.3),\n",
    "                        \"top_p\": kwargs.get(\"top_p\", 0.9),\n",
    "                        \"max_completion_tokens\": kwargs.get(\"max_completion_tokens\", 1024)\n",
    "                    })\n",
    "                )\n",
    "                response_body = json.loads(response['body'].read())\n",
    "                text = response_body['choices'][0]['message']['content']\n",
    "                generations.append([Generation(text=text)])\n",
    "            except Exception as e:\n",
    "                generations.append([Generation(text=f\"Error: {str(e)}\")])\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gpt-oss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaudeLLM(BaseLLM): # sonnet 4 \n",
    "    bedrock: Any = Field(exclude=True) \n",
    "    #model_id: str = \"anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "    model_id: str = \"arn:aws:bedrock:us-west-2:897189464960:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "    \n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            try:\n",
    "                response = self.bedrock.invoke_model(\n",
    "                    modelId=self.model_id,\n",
    "                    contentType=\"application/json\",\n",
    "                    accept=\"application/json\",\n",
    "                    body=json.dumps({\n",
    "                        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        \"temperature\": kwargs.get(\"temperature\", 0.3),\n",
    "                        \"top_p\": kwargs.get(\"top_p\", 0.9),\n",
    "                        \"max_tokens\": kwargs.get(\"max_tokens\", 1024)\n",
    "                    })\n",
    "                )\n",
    "\n",
    "                response_body = json.loads(response['body'].read())\n",
    "                text = response_body['content'][0]['text']\n",
    "                generations.append([Generation(text=text)])\n",
    "            except Exception as e:\n",
    "                generations.append([Generation(text=f\"Error: {str(e)}\")])\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"claude\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_llm(vectorstore_faiss, query):\n",
    "    # Initialize Bedrock client\n",
    "    session = boto3.Session(profile_name=\"Caleb\")\n",
    "    bedrock = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "    \n",
    "    # Initialize custom LLM\n",
    "    llm = GPTOSSLLM(bedrock=bedrock) \n",
    "    llm = ClaudeLLM(bedrock=bedrock) \n",
    "    \n",
    "    # Set up RetrievalQA\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore_faiss.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "\n",
    "    return qa.invoke({\"query\": query})[\"result\"]\n",
    "\n",
    "# def get_response_llm(vectorstore_faiss, query, model_id): # with selection\n",
    "#     session = boto3.Session(profile_name=\"Caleb\")\n",
    "#     bedrock = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "    \n",
    "#     if model_id.startswith(\"openai.\"):\n",
    "#         llm = GPTOSSLLM(bedrock=bedrock, model_id=model_id)\n",
    "#     elif model_id.startswith(\"anthropic.\"):\n",
    "#         llm = ClaudeLLM(bedrock=bedrock, model_id=model_id)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "    \n",
    "#     qa = RetrievalQA.from_chain_type(\n",
    "#         llm=llm,\n",
    "#         chain_type=\"stuff\",\n",
    "#         retriever=vectorstore_faiss.as_retriever(search_kwargs={\"k\": 3}),\n",
    "#         return_source_documents=True,\n",
    "#         chain_type_kwargs={\"prompt\": PROMPT}\n",
    "#     )\n",
    "\n",
    "#     return qa.invoke({\"query\": query})[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UserId': 'AROA5BZFIZOADUUEBUTW6:caleb.chan@nxp.com', 'Account': '897189464960', 'Arn': 'arn:aws:sts::897189464960:assumed-role/AWSReservedSSO_ccoe-powerusers_6dbe169efbadd99d/caleb.chan@nxp.com', 'ResponseMetadata': {'RequestId': '6625027f-9844-48ca-9e03-ce3d535329ab', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '6625027f-9844-48ca-9e03-ce3d535329ab', 'x-amz-sts-extended-request-id': 'MTp1cy1lYXN0LTE6MTc1NDU2NjM5OTI2OTpHOjlFcTlDMWJW', 'content-type': 'text/xml', 'content-length': '490', 'date': 'Thu, 07 Aug 2025 11:33:19 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "session = boto3.Session()\n",
    "bedrock = session.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "identity = sts.get_caller_identity()\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Claude, an AI assistant created by Anthropic. I don't have access to my exact model version information, but I'm designed to help with various tasks including the semiconductor test automation and script translation work you've set up for me.\n",
      "\n",
      "I can see from your system prompt that you'd like me to convert legacy MCT 2000 test scripts into modern SPEAL format while maintaining the original logic, structure, and intent. The context you've provided shows examples of semiconductor test scripts with digital pattern testing, pin configurations, and various test parameters.\n",
      "\n",
      "Is there a specific MCT 2000 script you'd like me to help translate to SPEAL format? I'm ready to assist with that conversion work.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\"\"\"\n",
    "       hello. what exact model vesion are you?\n",
    "\"\"\"\n",
    "#get_response_llm(faiss_index, user_input)\n",
    "response_text = get_response_llm(faiss_index, user_input)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-sonnet-4-20250514-v1:0 with on-demand throughput isn‚Äôt supported. Retry your request with the ID or ARN of an inference profile that contains this model.'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = 'What is this document about?'\n",
    "get_response_llm(faiss_index, user_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from langchain_community.retrievers import AmazonKnowledgeBasesRetriever\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "bedrock = boto3.client(\"bedrock-agent\", region_name=\"us-west-1\")  # Adjust region\n",
    "kb_name = \"mct-to-speal-kb\"\n",
    "s3_uri = \"s3://your-bucket/mct-docs/\"  # already uploaded chunked .txt files\n",
    "bedrock_role_arn = \"arn:aws:iam::<your-account>:role/BedrockKnowledgeBaseRole\"\n",
    "\n",
    "# === STEP 1: Create or describe knowledge base ===\n",
    "response = bedrock.create_knowledge_base(\n",
    "    name=kb_name,\n",
    "    description=\"KB for MCT to SPEAL code conversion\",\n",
    "    roleArn=bedrock_role_arn,\n",
    "    knowledgeBaseConfiguration={\n",
    "        \"type\": \"VECTOR\",\n",
    "        \"vectorKnowledgeBaseConfiguration\": {\n",
    "            \"embeddingModelArn\": \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1\"\n",
    "        }\n",
    "    },\n",
    "    storageConfiguration={\n",
    "        \"type\": \"S3\",\n",
    "        \"s3\": {\n",
    "            \"bucketArn\": \"arn:aws:s3:::your-bucket\",\n",
    "            \"inclusionPrefixes\": [\"mct-docs/\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "knowledge_base_id = response[\"knowledgeBase\"][\"knowledgeBaseId\"]\n",
    "print(\"Created KB:\", knowledge_base_id)\n",
    "\n",
    "# === STEP 2: Use LangChain to connect to KB ===\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=knowledge_base_id,\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# === STEP 3: Set up Claude via Bedrock ===\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region_name=\"us-east-1\",\n",
    "    model_kwargs={\"temperature\": 0.1}\n",
    ")\n",
    "\n",
    "# === STEP 4: Retrieval QA chain ===\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# === STEP 5: Ask a question ===\n",
    "query = \"How do I convert an MCT loop to a SPEAL FOR structure?\"\n",
    "response = rag_chain({\"query\": query})\n",
    "\n",
    "print(\"Answer:\", response[\"result\"])\n",
    "print(\"Sources:\", [doc.metadata[\"source\"] for doc in response[\"source_documents\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
